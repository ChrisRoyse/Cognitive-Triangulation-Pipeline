# Key Research Questions

This document outlines the central questions that will guide the research for the "Cognitive Triangulation" code analysis pipeline. These questions are derived from the project's core objective: to build a code analysis system that relies exclusively on LLMs.

## 1. `EntityScout`: LLM-based Entity Recognition

*   **Q1.1**: What are the most effective prompt engineering techniques for instructing an LLM to perform a "shallow scan" of a code file and identify a predefined set of "Points of Interest" (POIs) like functions, classes, variables, and external calls?
*   **Q1.2**: How can we optimize for speed and cost in this initial scanning phase? Should we use smaller, faster models for `EntityScout`? What is the trade-off in accuracy?
*   **Q1.3**: What is the most effective and structured format (e.g., JSON, XML) for the POI reports generated by `EntityScout` to ensure they are easily consumable by the `RelationshipResolver`?
*   **Q1.4**: How can the LLM be instructed to handle different programming languages and their syntactic variations for entity recognition without relying on traditional parsers?

## 2. `RelationshipResolver`: Cross-file Relationship Analysis

*   **Q2.1**: What are the state-of-the-art methods for an LLM to analyze a collection of POIs from multiple files and accurately infer complex relationships (e.g., function `A` calls function `B`, class `X` inherits from class `Y`)?
*   **Q2.2**: How can we manage the context limitations of LLMs when the entire codebase's POI data exceeds the context window? What are the best strategies for chunking or summarizing context?
*   **Q2.3**: What data structures or representations of POIs are most effective for feeding into the `RelationshipResolver`'s LLM to facilitate relationship detection?

## 3. "Cognitive Triangulation" Strategies

*   **Q3.1**: What specific "Cognitive Triangulation" strategies can be employed to validate the findings of the LLMs and reduce hallucinations?
    *   *Examples*: Using multiple different LLM models (e.g., GPT, Claude, Gemini) to cross-reference results? Using the same model with different prompts (e.g., one focused on finding calls, another on confirming them)? A multi-pass approach where an LLM refines its own previous analysis?
*   **Q3.2**: How can we design a "confidence score" for entities and relationships based on the outcomes of triangulation?
*   **Q3.3**: What is the optimal workflow for this triangulation? Should `RelationshipResolver` propose relationships that are then validated, or should it be a more integrated process?

## 4. `GraphBuilder`: Populating the Graph Database

*   **Q4.1**: What is the most robust method for mapping the validated entities and relationships from the `RelationshipResolver` into a Neo4j graph structure?
*   **Q4.2**: How can we ensure the process of populating the database is idempotent to prevent duplicate nodes or relationships on subsequent runs?

## 5. General Challenges and Mitigations

*   **Q5.1**: What are the primary performance bottlenecks expected in an entirely LLM-driven pipeline, and what are the mitigation strategies (e.g., parallelization, batching, model quantization)?
*   **Q5.2**: How can the system handle syntactic ambiguity or errors in the source code gracefully, given the absence of a deterministic parser to flag them?
*   **Q5.3**: What are the best practices for structuring prompts to elicit structured, parseable output (like JSON) from LLMs reliably?
*   **Q5.4**: How can we build a continuous evaluation and improvement loop for the pipeline, potentially using a ground-truth dataset to measure accuracy and recall over time?