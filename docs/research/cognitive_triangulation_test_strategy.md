# Cognitive Triangulation -- High-Level Test Strategy

## 1. Introduction

This document outlines the high-level acceptance testing strategy for the Cognitive Triangulation architectural refactor. The goal of this strategy is to ensure the successful implementation of the features and improvements detailed in the [`cognitive_triangulation_improvement_plan.md`](docs/architecture/cognitive_triangulation_improvement_plan.md) and validated against the user outcomes defined in [`cognitive_triangulation_user_stories.md`](docs/specifications/user_stories/cognitive_triangulation_user_stories.md).

This strategy adheres to the "Simplicity-First" approach recommended in the [`cognitive_triangulation_strategy_report.md`](docs/research/cognitive_triangulation_strategy_report.md), focusing on pragmatic and high-impact tests that validate the core principles of the refactor-- accuracy, reliability, and observability. This plan will serve as the foundation for the new Master Acceptance Test Plan.

## 2. Testing Focus -- Critical Areas for Validation

Based on the architectural goals and user stories, our testing efforts must be concentrated on the following four critical areas to validate the success of the refactor.

### 2.1. Accuracy of Confidence Scoring & Evidence
This is the cornerstone of the refactor. We must verify that the new confidence scores are not just present, but meaningful.
-- **Core Functionality**: Confirm that every relationship generated by the pipeline has an associated confidence score, as per User Story 1.1.
-- **Evidence Trail**: Ensure that the "evidence" for each score (i.e., the agreement or disagreement between different analysis passes) is auditable and accessible, fulfilling User Story 1.2.
-- **Score Calibration**: Validate that the scores genuinely reflect the system's certainty--high for clear-cut relationships, low for ambiguous ones.

### 2.2. Correctness of Cross-Validation Logic
The shift from a sequential pipeline to true triangulation is validated here. We need to test the "peer-review" mechanism where agents validate each other's findings.
-- **Agreement Boost**: Verify that when multiple analysis passes agree on a relationship, its confidence score is significantly boosted (User Story 2.1).
-- **Disagreement Penalty**: Verify that when agents disagree, the conflict is logged (User Story 2.2) and the final confidence score is appropriately lowered.
-- **End-to-End Integration**: Confirm the complete, refactored pipeline now successfully persists the final, confidence-scored graph to Neo4j, closing the gap identified in the `initial_cognitive_triangulation_comprehension_report.md`.

### 2.3. System Resilience Under Failure
A production-grade system must be robust. We will proactively test the new resilience patterns.
-- **Transient Error Handling**: Confirm that the system can withstand transient errors from external services (like the LLM API) by using the implemented retry logic (User Story 3.1).
-- **Circuit Breaker Engagement**: Ensure the circuit breaker pattern works as expected, tripping after repeated failures and preventing cascading failures throughout the system.

### 2.4. Observability and Status Monitoring
The system's inner workings must be transparent to users and operators.
-- **Health Checks**: Validate the liveness and readiness endpoints for all core services (User Story 3.2).
-- **Job Status Dashboard**: Confirm that the analysis status dashboard is accurate, providing a real-time view of the job hierarchy and the status of each stage.

## 3. Methodology

A multi-faceted testing methodology is required to cover the critical focus areas effectively.

-- **End-to-End (E2E) Acceptance Testing**: This will be our primary methodology. Full pipeline runs will be executed on curated test repositories. We will assert the final state of the Neo4j graph, the contents of the SQLite database, and structured logs to validate the majority of the user stories from a black-box perspective.

-- **Integration Testing (Agent Collaboration)**: To test the peer-review mechanism in isolation, we will create focused integration tests. These tests will trigger a limited set of agents (e.g., `FileAnalysisWorker` and `DirectoryResolutionWorker`) and inspect the intermediate data to ensure their interaction and data reconciliation logic is correct without running the entire pipeline.

-- **Chaos Testing (Failure Injection)**: To validate resilience, we will deliberately inject failures. This involves creating mocks for services like the `deepseekClient` that can be configured to return transient errors (e.g., HTTP 503) or to time out. We will then assert that the system's retry logic and circuit breakers behave as specified in User Story 3.1.

-- **Component Testing**: While the focus is high-level, new, critical, and isolated logic units, such as the confidence score calculation algorithm and the circuit breaker state machine, will be validated with dedicated unit tests to ensure their internal correctness.

## 4. Key Metrics for Success

Success will be measured against specific, quantifiable metrics that are directly tied to the project's goals.

| Category                      | Metric                                                                                   | Target                                 |
| ----------------------------- | ---------------------------------------------------------------------------------------- | -------------------------------------- |
| **Confidence & Accuracy**     | Correlation between system confidence scores and human-annotated "ground truth" data.      | Spearman Correlation > 0.7             |
|                               | Percentage of known "false positives" correctly assigned a low confidence score (<30%).    | > 80%                                  |
|                               | Percentage of known "true positives" receiving a score boost from agent agreement.       | > 90%                                  |
| **Resilience**                | Successful pipeline completion rate when subjected to a 5% transient LLM error rate.       | > 99%                                  |
|                               | Time-to-recovery for a tripped circuit breaker.                                          | Recovers within the configured window. |
| **Performance**               | Performance overhead introduced by the refactor on a benchmark repository.               | < 15% increase in total run time.      |
| **Observability**             | Uptime and response time of `/health/readiness` and `/health/liveness` endpoints.        | 100% uptime during test runs.          |

## 5. Test Data Requirements

Effective validation requires a diverse and well-structured set of test data. We will use several types of repositories for testing.

-- **"Ground Truth" Repository**: A small, human-vetted codebase with manually annotated POIs and relationships. This will be the primary dataset for measuring the accuracy and calibration of confidence scores.

-- **"Ambiguity" Repository**: A specially crafted codebase containing deliberately confusing patterns, such as homonymous functions, subtle dependencies, and unconventional coding styles. This repository is essential for testing how the confidence scoring and cross-validation logic handle uncertainty.

-- **"Polyglot" Repository**: The existing `polyglot-test` project will be used and potentially expanded to test the system's ability to handle relationships that span multiple programming languages.

-- **"Large-Scale" Repository**: A medium-sized, real-world open-source project. This will be used for performance and scalability testing, and to uncover edge cases that only appear at scale.

-- **"Malformed" Repository**: A codebase containing files with syntax errors. This will test the system's robustness and ensure it can gracefully handle and log errors for un-parseable files without crashing the entire analysis.
