# Master Acceptance Test Plan -- Universal Code Graph V3

## 1. Objectives

The primary objective of this acceptance test plan is to verify that the Universal Code Graph V3 pipeline meets its core mission-- to create a scalable, deterministic, and resilient system that accurately transforms any software codebase into a rich, queryable knowledge graph.

The tests outlined here will serve as the ultimate success criteria for the project. All subsequent development and refinement work will be aimed at making these tests pass.

## 2. Scope

These tests will cover the complete, end-to-end functionality of the pipeline, treating the system as a black box. The scope includes the following key user stories and system flows--

- **Initial Repository Processing**: Verifying the system's ability to correctly process a "greenfield" repository from a clean slate.
- **Incremental Repository Updates**: Verifying the system's ability to accurately detect and process changes, including file additions, modifications, deletions, and renames.
- **Graph Correctness & Fidelity**: Verifying that the final Neo4j graph precisely matches a pre-defined "golden" state for a given sample codebase.
- **Error Resilience & Recovery**: Verifying the system's robustness against predictable failures, such as LLM API errors, malformed data, and network timeouts.

## 3. Test Environment Requirements

A consistent and isolated test environment is required for executing these acceptance tests.

- **Orchestration**: An automated test runner (e.g., Vitest, Jest) capable of executing scripts.
- **Databases**:
    - A running, accessible Neo4j instance that can be programmatically cleared before each test run.
    - A local file system path for the SQLite database, which will be deleted and recreated for each test run.
- **Sample Data**:
    - A `test-data` directory containing several small, clean, and representative sample code repositories.
    - A corresponding set of "golden" graph files (e.g., in Cypher format) that define the expected final state of the Neo4j graph for each sample repository.
- **Mock Services**:
    - A mock LLM API server that can be configured to return successful responses, invalid JSON, HTTP error codes, and simulated timeouts to test the `WorkerAgent`'s resilience.

## 4. AI-Verifiable Success Criteria

The entire acceptance test suite is considered "passed" when 100% of the individual test cases pass. Each test case has its own set of AI-verifiable criteria, meaning a program can determine the outcome without human intervention.

A test case is successful if and only if all of the following conditions are met after the pipeline has run--

1.  **SQLite State Verification**:
    - `work_queue` Table-- All initial tasks must have a status of `completed`.
    - `analysis_results` Table-- All result records must have a status of `ingested`.
    - `refactoring_tasks` Table-- All refactoring tasks must have a status of `completed`.
    - `failed_work` Table-- This "dead-letter" table must be empty (except in specific failure-resilience tests).

2.  **Neo4j Graph State Verification**:
    - The graph generated by the pipeline must be structurally and semantically identical to the "golden" graph defined for the input test data. This is verified by--
        - Executing a series of Cypher queries to check node counts by label (`:File`, `:Function`, etc.).
        - Checking relationship counts by type (`:CALLS`, `:IMPORTS`, etc.).
        - Asserting the existence of specific, critical paths and properties within the graph (e.g., a specific function `CALLS` another).

## 5. Automation Strategy

The acceptance tests will be implemented as automated scripts. A typical test script will perform the following actions--

1.  **Setup**:
    - Clean the Neo4j database.
    - Delete the old SQLite database file.
    - Prepare the sample code repository on the file system.
    - Configure and start the mock LLM API if required for the test.
2.  **Execution**:
    - Trigger the `ScoutAgent` to scan the repository and populate the initial queues.
    - Trigger the `WorkerAgent` pool to process the work queue.
    - Trigger the `GraphIngestorAgent` to build the graph.
3.  **Verification**:
    - Connect to the SQLite database and run queries to assert the final state of all tables as defined in the Success Criteria.
    - Connect to the Neo4j database and run Cypher queries to compare the resulting graph against the golden standard.
4.  **Teardown**:
    - Stop any mock services.